# Soldier Default Configuration
# This file contains sensible defaults for all configuration options.
# Environment-specific files override these values.

app_name = "soldier"
debug = false
log_level = "INFO"

# =============================================================================
# API Configuration
# =============================================================================
[api]
host = "0.0.0.0"
port = 8000
workers = 4
cors_origins = ["*"]
cors_allow_credentials = true

[api.rate_limit]
enabled = true
requests_per_minute = 60
burst_size = 10

# =============================================================================
# Storage Configuration
# =============================================================================
[storage.config]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.memory]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.session]
backend = "redis"
pool_size = 10
pool_timeout = 30

[storage.audit]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.vector]
backend = "qdrant"
collection_prefix = "soldier"
dimensions = 1024
distance_metric = "cosine"
timeout = 60.0
# Embedding storage strategy
store_on_entity = true       # Store embedding on entity record (rule.embedding)
sync_to_vector_store = true  # Sync to vector store for similarity search

# =============================================================================
# AI Provider Configuration
# =============================================================================
# Model format determines the provider automatically:
#   - "openrouter/anthropic/claude-3-haiku-20240307" -> OpenRouter
#   - "anthropic/claude-3-haiku-20240307" -> Direct Anthropic API
#   - "openai/gpt-4o-mini" -> Direct OpenAI API
#
# API Keys (set in environment):
#   - OPENROUTER_API_KEY: For openrouter/* models
#   - ANTHROPIC_API_KEY: For anthropic/* models (direct fallback)
#   - OPENAI_API_KEY: For openai/* models (direct fallback)

[providers]
default_embedding = "default"
default_rerank = "default"

[providers.embedding.default]
provider = "jina"
model = "jina-embeddings-v3"
dimensions = 1024
batch_size = 100

[providers.rerank.default]
provider = "jina"
model = "jina-reranker-v2-base-multilingual"
top_k = 10

# =============================================================================
# Pipeline Configuration
# =============================================================================
# Each step specifies its model directly with optional fallbacks.
# Model strings include the aggregator prefix when using OpenRouter.
#
# OpenRouter Provider Routing (optional per-step):
#   Each step can have an [pipeline.{step}.openrouter] section to control
#   which OpenRouter provider handles the request:
#
#   [pipeline.generation.openrouter]
#   provider_order = ["Anthropic", "Together", "Fireworks"]  # Try in this order
#   provider_sort = "price"          # Or "latency" or "throughput"
#   allow_fallbacks = true           # Allow other providers if listed ones fail
#   ignore_providers = ["SomeProvider"]  # Never use these providers
#
#   See: https://openrouter.ai/docs#provider-routing

[pipeline.context_extraction]
enabled = true
mode = "llm"
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []
history_turns = 5
# Optional: uncomment to configure OpenRouter provider routing for this step
# [pipeline.context_extraction.openrouter]
# provider_sort = "latency"  # Prioritize fastest providers for context extraction

[pipeline.retrieval]
enabled = true
embedding_provider = "default"
max_k = 30

[pipeline.retrieval.rule_selection]
strategy = "adaptive_k"
min_score = 0.5
max_k = 10

[pipeline.retrieval.scenario_selection]
strategy = "entropy"
min_score = 0.5
max_k = 5

[pipeline.retrieval.memory_selection]
strategy = "clustering"
min_score = 0.4
max_k = 15

[pipeline.reranking]
enabled = true
rerank_provider = "default"
top_k = 10

[pipeline.rule_filtering]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []
history_turns = 5
batch_size = 5

[pipeline.scenario_filtering]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []


[pipeline.generation]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []
temperature = 0.7
max_tokens = 1024

[pipeline.enforcement]
enabled = true
self_critique_enabled = false
max_retries = 2

# =============================================================================
# Memory Ingestion Configuration
# =============================================================================
[pipeline.memory_ingestion]
enabled = true
embedding_enabled = true
entity_extraction_enabled = true
summarization_enabled = true
async_extraction = true
async_summarization = true
queue_backend = "redis"
max_ingestion_latency_ms = 500

[pipeline.memory_ingestion.entity_extraction]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 1024
temperature = 0.3
batch_size = 10
timeout_ms = 2000
min_confidence = "medium"

[pipeline.memory_ingestion.deduplication]
exact_match_enabled = true
fuzzy_match_enabled = true
fuzzy_threshold = 0.85
embedding_match_enabled = true
embedding_threshold = 0.80
rule_based_enabled = true

[pipeline.memory_ingestion.summarization.window]
turns_per_summary = 20
model = "openrouter/anthropic/claude-3.5-haiku"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 256
temperature = 0.5

[pipeline.memory_ingestion.summarization.meta]
summaries_per_meta = 5
enabled_at_turn_count = 100
model = "openrouter/anthropic/claude-3.5-haiku"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 512
temperature = 0.5

# =============================================================================
# Selection Strategies
# =============================================================================
[selection.rule]
strategy = "adaptive_k"
min_score = 0.5
max_k = 10

[selection.scenario]
strategy = "entropy"
min_score = 0.5
max_k = 5

[selection.memory]
strategy = "clustering"
min_score = 0.4
max_k = 15

# =============================================================================
# Observability Configuration
# =============================================================================
[observability.logging]
level = "INFO"
format = "json"
include_trace_id = true

[observability.tracing]
enabled = true
service_name = "soldier"
sample_rate = 1.0

[observability.metrics]
enabled = true
port = 9090
path = "/metrics"

# =============================================================================
# Scenario Migration Configuration
# =============================================================================
[scenario_migration]
enabled = true

[scenario_migration.deployment]
auto_mark_sessions = true
require_approval = true

[scenario_migration.gap_fill]
extraction_enabled = true
extraction_confidence_threshold = 0.85
confirmation_threshold = 0.95
max_conversation_turns = 20

[scenario_migration.re_routing]
enabled = true
notify_user = true
notification_template = "I have new instructions. Let me redirect our conversation."

[scenario_migration.checkpoints]
block_teleport_past_checkpoint = true
log_checkpoint_blocks = true

[scenario_migration.retention]
version_retention_days = 7
plan_retention_days = 30

[scenario_migration.logging]
log_clean_grafts = false
log_gap_fills = true
log_re_routes = true
log_checkpoint_blocks = true

# =============================================================================
# Background Jobs Configuration (Hatchet)
# =============================================================================
[jobs.hatchet]
enabled = true
server_url = "http://localhost:7077"
# api_key loaded from HATCHET_API_KEY env var
worker_concurrency = 10
cron_expire_fields = "*/5 * * * *"
cron_detect_orphans = "*/15 * * * *"
cron_schema_extraction = ""
retry_max_attempts = 3
retry_backoff_seconds = 60
