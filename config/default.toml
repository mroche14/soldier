# Soldier Default Configuration
# This file contains sensible defaults for all configuration options.
# Environment-specific files override these values.

app_name = "soldier"
debug = false
log_level = "INFO"

# =============================================================================
# API Configuration
# =============================================================================
[api]
host = "0.0.0.0"
port = 8000
workers = 4
cors_origins = ["*"]
cors_allow_credentials = true

[api.rate_limit]
enabled = true
requests_per_minute = 60
burst_size = 10

# =============================================================================
# Storage Configuration
# =============================================================================
[storage.config]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.memory]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.session]
backend = "redis"
pool_size = 10
pool_timeout = 30

[storage.audit]
backend = "postgres"
pool_size = 10
pool_timeout = 30

# =============================================================================
# AI Provider Configuration
# =============================================================================
[providers]
default_llm = "haiku"
default_embedding = "default"
default_rerank = "default"

[providers.llm.haiku]
provider = "anthropic"
model = "claude-3-haiku-20240307"
max_tokens = 4096
temperature = 0.7
timeout = 60

[providers.llm.sonnet]
provider = "anthropic"
model = "claude-sonnet-4-5-20250514"
max_tokens = 4096
temperature = 0.7
timeout = 120

[providers.embedding.default]
provider = "openai"
model = "text-embedding-3-small"
dimensions = 1536
batch_size = 100

[providers.rerank.default]
provider = "cohere"
model = "rerank-english-v3.0"
top_k = 10

# =============================================================================
# Pipeline Configuration
# =============================================================================
[pipeline.context_extraction]
enabled = true
mode = "llm"
llm_provider = "haiku"
history_turns = 5

[pipeline.retrieval]
enabled = true
embedding_provider = "default"
max_k = 30

[pipeline.retrieval.rule_selection]
strategy = "adaptive_k"
min_score = 0.5
max_k = 10

[pipeline.retrieval.scenario_selection]
strategy = "entropy"
min_score = 0.5
max_k = 5

[pipeline.retrieval.memory_selection]
strategy = "clustering"
min_score = 0.4
max_k = 15

[pipeline.reranking]
enabled = true
rerank_provider = "default"
top_k = 10

[pipeline.llm_filtering]
enabled = true
llm_provider = "haiku"
batch_size = 5

[pipeline.generation]
enabled = true
llm_provider = "sonnet"
temperature = 0.7
max_tokens = 1024

[pipeline.enforcement]
enabled = true
self_critique_enabled = false
max_retries = 2

# =============================================================================
# Memory Ingestion Configuration
# =============================================================================
[pipeline.memory_ingestion]
enabled = true
embedding_enabled = true
entity_extraction_enabled = true
summarization_enabled = true
async_extraction = true
async_summarization = true
queue_backend = "inmemory"
max_ingestion_latency_ms = 500

[pipeline.memory_ingestion.entity_extraction]
enabled = true
llm_provider = "anthropic"
model = "haiku"
max_tokens = 1024
temperature = 0.3
batch_size = 10
timeout_ms = 2000
min_confidence = "medium"

[pipeline.memory_ingestion.deduplication]
exact_match_enabled = true
fuzzy_match_enabled = true
fuzzy_threshold = 0.85
embedding_match_enabled = true
embedding_threshold = 0.80
rule_based_enabled = true

[pipeline.memory_ingestion.summarization.window]
turns_per_summary = 20
llm_provider = "anthropic"
model = "haiku"
max_tokens = 256
temperature = 0.5

[pipeline.memory_ingestion.summarization.meta]
summaries_per_meta = 5
enabled_at_turn_count = 100
llm_provider = "anthropic"
model = "haiku"
max_tokens = 512
temperature = 0.5

# =============================================================================
# Selection Strategies
# =============================================================================
[selection.rule]
strategy = "adaptive_k"
min_score = 0.5
max_k = 10

[selection.scenario]
strategy = "entropy"
min_score = 0.5
max_k = 5

[selection.memory]
strategy = "clustering"
min_score = 0.4
max_k = 15

# =============================================================================
# Observability Configuration
# =============================================================================
[observability.logging]
level = "INFO"
format = "json"
include_trace_id = true

[observability.tracing]
enabled = true
service_name = "soldier"
sample_rate = 1.0

[observability.metrics]
enabled = true
port = 9090
path = "/metrics"

# =============================================================================
# Scenario Migration Configuration
# =============================================================================
[scenario_migration]
enabled = true

[scenario_migration.deployment]
auto_mark_sessions = true
require_approval = true

[scenario_migration.gap_fill]
extraction_enabled = true
extraction_confidence_threshold = 0.85
confirmation_threshold = 0.95
max_conversation_turns = 20

[scenario_migration.re_routing]
enabled = true
notify_user = true
notification_template = "I have new instructions. Let me redirect our conversation."

[scenario_migration.checkpoints]
block_teleport_past_checkpoint = true
log_checkpoint_blocks = true

[scenario_migration.retention]
version_retention_days = 7
plan_retention_days = 30

[scenario_migration.logging]
log_clean_grafts = false
log_gap_fills = true
log_re_routes = true
log_checkpoint_blocks = true
