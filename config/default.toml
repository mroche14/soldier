# Focal Default Configuration
# This file contains sensible defaults for all configuration options.
# Environment-specific files override these values.

app_name = "focal"
debug = false
log_level = "INFO"

# =============================================================================
# API Configuration
# =============================================================================
[api]
host = "0.0.0.0"
port = 8000
workers = 4
cors_origins = ["*"]
cors_allow_credentials = true

[api.rate_limit]
enabled = true
requests_per_minute = 60
burst_size = 10

# =============================================================================
# Storage Configuration
# =============================================================================
[storage.config]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.memory]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.session]
backend = "redis"
pool_size = 10
pool_timeout = 30

[storage.audit]
backend = "postgres"
pool_size = 10
pool_timeout = 30

[storage.vector]
backend = "qdrant"
collection_prefix = "focal"
dimensions = 1024
distance_metric = "cosine"
timeout = 60.0
# Embedding storage strategy
store_on_entity = true       # Store embedding on entity record (rule.embedding)
sync_to_vector_store = true  # Sync to vector store for similarity search

# =============================================================================
# AI Provider Configuration
# =============================================================================
# Model format determines the provider automatically:
#   - "openrouter/anthropic/claude-3-haiku-20240307" -> OpenRouter
#   - "anthropic/claude-3-haiku-20240307" -> Direct Anthropic API
#   - "openai/gpt-4o-mini" -> Direct OpenAI API
#
# API Keys (set in environment):
#   - OPENROUTER_API_KEY: For openrouter/* models
#   - ANTHROPIC_API_KEY: For anthropic/* models (direct fallback)
#   - OPENAI_API_KEY: For openai/* models (direct fallback)

[providers]
default_embedding = "default"
default_rerank = "default"

[providers.embedding.default]
provider = "jina"
model = "jina-embeddings-v3"
dimensions = 1024
batch_size = 100

[providers.rerank.default]
provider = "jina"
model = "jina-reranker-v2-base-multilingual"
top_k = 10

# =============================================================================
# Pipeline Configuration
# =============================================================================

# Phase 1: Turn Context Loading
[pipeline.turn_context]
load_glossary = true
load_customer_data_schema = true
enable_scenario_reconciliation = true

# Phase 3: Customer Data Update
[pipeline.customer_data_update]
enabled = true
validation_mode = "strict"  # strict, warn, disabled
max_history_entries = 10    # Max history items per variable

# Glossary Configuration
[glossary]
enabled = true
max_items_per_turn = 50  # Limit to avoid token bloat
external_source = ""  # e.g., "s3://bucket/glossary.json"

# Customer Data Configuration
[customer_data]
enabled = true
session_scope_ttl_minutes = 120  # SESSION-scoped variables expire after 2 hours
strict_validation = false  # If true, reject invalid values; if false, warn
# Each step specifies its model directly with optional fallbacks.
# Model strings include the aggregator prefix when using OpenRouter.
#
# OpenRouter Provider Routing (optional per-step):
#   Each step can have an [pipeline.{step}.openrouter] section to control
#   which OpenRouter provider handles the request:
#
#   [pipeline.generation.openrouter]
#   provider_order = ["Anthropic", "Together", "Fireworks"]  # Try in this order
#   provider_sort = "price"          # Or "latency" or "throughput"
#   allow_fallbacks = true           # Allow other providers if listed ones fail
#   ignore_providers = ["SomeProvider"]  # Never use these providers
#
#   See: https://openrouter.ai/docs#provider-routing

[pipeline.situational_sensor]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq", "google-vertex", "sambanova"]
provider_sort = "latency"
allow_fallbacks = true
ignore_providers = []
temperature = 0.0  # Deterministic for extraction
max_tokens = 800
history_turns = 5  # Configurable K (currently hardcoded)
include_glossary = true
include_schema_mask = true

[pipeline.retrieval]
enabled = true
embedding_provider = "default"
max_k = 30

[pipeline.retrieval.rule_selection]
strategy = "adaptive_k"
min_score = 0.5
max_k = 10

[pipeline.retrieval.scenario_selection]
strategy = "entropy"
min_score = 0.5
max_k = 5

[pipeline.retrieval.memory_selection]
strategy = "clustering"
min_score = 0.4
max_k = 15

[pipeline.retrieval.intent_selection]
strategy = "elbow"
min_score = 0.6
max_k = 5

# Per-object-type reranking configuration
[pipeline.retrieval.rule_reranking]
enabled = true
rerank_provider = "default"
top_k = 20

[pipeline.retrieval.scenario_reranking]
enabled = false  # Optional - enable if needed

[pipeline.retrieval.memory_reranking]
enabled = true
rerank_provider = "default"
top_k = 10

[pipeline.retrieval.intent_reranking]
enabled = true
rerank_provider = "default"
top_k = 10

# Hybrid (vector + BM25) retrieval configuration per object type
[pipeline.retrieval.rule_hybrid]
enabled = false
vector_weight = 0.7
bm25_weight = 0.3
normalization = "min_max"

[pipeline.retrieval.scenario_hybrid]
enabled = false
vector_weight = 0.7
bm25_weight = 0.3
normalization = "min_max"

[pipeline.retrieval.memory_hybrid]
enabled = false
vector_weight = 0.7
bm25_weight = 0.3
normalization = "min_max"

[pipeline.retrieval.intent_hybrid]
enabled = false
vector_weight = 0.7
bm25_weight = 0.3
normalization = "min_max"

[pipeline.rule_filtering]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []
history_turns = 5
batch_size = 5
confidence_threshold = 0.7               # Minimum confidence for APPLIES
unsure_policy = "exclude"                # "include" | "exclude" | "log_only"

[pipeline.relationship_expansion]
enabled = true
max_depth = 2                            # Maximum relationship chain depth

[pipeline.scenario_filtering]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []

# Phase 6: Scenario Orchestration
[pipeline.scenario_orchestration]
enabled = true
max_loop_count = 3  # Maximum visits to a step before triggering relocalization
max_simultaneous_scenarios = 5  # Limit active scenarios per session
block_on_missing_hard_fields = true  # Block scenario entry when hard requirements missing
enable_step_skipping = true  # Enable automatic step skipping with available data
enable_multi_scenario = true  # Allow multiple active scenarios

# Phase 7: Tool Execution
[pipeline.tool_execution]
enabled = true
timeout_ms = 5000  # Timeout per tool execution
max_parallel = 5  # Max tools to execute in parallel
fail_fast = false  # Stop on first tool failure
enable_before_step = true  # Enable BEFORE_STEP tool execution
enable_during_step = true  # Enable DURING_STEP tool execution
enable_after_step = true  # Enable AFTER_STEP tool execution

# Phase 8: Response Planning
[pipeline.response_planning]
enabled = true
prioritize_escalation = true  # Escalation rules override all other types
merge_templates = true  # Combine templates from multiple scenarios
extract_must_include = true  # Extract must_include constraints from rules
extract_must_avoid = true  # Extract must_avoid constraints from rules
sort_by_urgency = true  # Sort contributions by urgency before scenario order

[pipeline.generation]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
provider_order = ["cerebras", "groq","google-vertex","sambanova"]  # Try Anthropic first
provider_sort = "latency"                # Or "price" or "throughput"
allow_fallbacks = true                   # Allow other providers if listed fail
ignore_providers = []
temperature = 0.7
max_tokens = 1024

# Structured output for categories
structured_output = true

# Channel formatting
[pipeline.generation.channels]
whatsapp_max_length = 4096
sms_max_length = 160
email_add_signature = true

[pipeline.enforcement]
enabled = true
self_critique_enabled = false
max_retries = 2

# =============================================================================
# Memory Ingestion Configuration
# =============================================================================
[pipeline.memory_ingestion]
enabled = true
embedding_enabled = true
entity_extraction_enabled = true
summarization_enabled = true
async_extraction = true
async_summarization = true
queue_backend = "redis"
max_ingestion_latency_ms = 500

[pipeline.memory_ingestion.entity_extraction]
enabled = true
model = "openrouter/openai/gpt-oss-120b"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 1024
temperature = 0.3
batch_size = 10
timeout_ms = 2000
min_confidence = "medium"

[pipeline.memory_ingestion.deduplication]
exact_match_enabled = true
fuzzy_match_enabled = true
fuzzy_threshold = 0.85
embedding_match_enabled = true
embedding_threshold = 0.80
rule_based_enabled = true

[pipeline.memory_ingestion.summarization.window]
turns_per_summary = 20
model = "openrouter/anthropic/claude-3.5-haiku"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 256
temperature = 0.5

[pipeline.memory_ingestion.summarization.meta]
summaries_per_meta = 5
enabled_at_turn_count = 100
model = "openrouter/anthropic/claude-3.5-haiku"
fallback_models = ["anthropic/claude-3-5-haiku-20241022"]
max_tokens = 512
temperature = 0.5


# =============================================================================
# Observability Configuration
# =============================================================================
[observability.logging]
level = "INFO"
format = "json"
include_trace_id = true

[observability.tracing]
enabled = true
service_name = "focal"
sample_rate = 1.0

[observability.metrics]
enabled = true
port = 9090
path = "/metrics"

# =============================================================================
# Scenario Migration Configuration
# =============================================================================
[scenario_migration]
enabled = true

[scenario_migration.deployment]
auto_mark_sessions = true
require_approval = true

[scenario_migration.gap_fill]
extraction_enabled = true
extraction_confidence_threshold = 0.85
confirmation_threshold = 0.95
max_conversation_turns = 20

[scenario_migration.re_routing]
enabled = true
notify_user = true
notification_template = "I have new instructions. Let me redirect our conversation."

[scenario_migration.checkpoints]
block_teleport_past_checkpoint = true
log_checkpoint_blocks = true

[scenario_migration.retention]
version_retention_days = 7
plan_retention_days = 30

[scenario_migration.logging]
log_clean_grafts = false
log_gap_fills = true
log_re_routes = true
log_checkpoint_blocks = true

# =============================================================================
# Background Jobs Configuration (Hatchet)
# =============================================================================
[jobs.hatchet]
enabled = true
server_url = "http://localhost:7077"
# api_key loaded from HATCHET_API_KEY env var
worker_concurrency = 10
cron_expire_fields = "*/5 * * * *"
cron_detect_orphans = "*/15 * * * *"
cron_schema_extraction = ""
retry_max_attempts = 3
retry_backoff_seconds = 60
